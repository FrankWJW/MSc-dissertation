{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook is to visualise the maximum activation of the filter\n",
    "Model wil use MLP with conditional time series data\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from keras.layers import Dense, Dropout, Activation, LSTM, Convolution1D, MaxPooling1D, Flatten\n",
    "# from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from torchbearer import Trial\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchbearer\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "from torchbearer import Trial\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define baseline MLP model\n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)        \n",
    "        out = self.fc2(out)\n",
    "        out = F.sigmoid(out)\n",
    "        out = out.squeeze()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model and load state\n",
    "model = BaselineModel()\n",
    "model.load_state_dict(torch.load('mlp_model'))\n",
    "\n",
    "# put model in eval mode\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gradient_generator():\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        \n",
    "        self.loss = nn.BCELoss()\n",
    "        self.model = model.to('cpu')\n",
    "        self.gradients = None\n",
    "        # Put model in evaluation mode\n",
    "        self.model.eval()\n",
    "        # Hook the first layer to get the gradient\n",
    "        self.hook = self.hook_layers()\n",
    "    def hook_layers(self):    \n",
    "        def hook_function(module, grad_in, grad_out):\n",
    "            print(\"hook called\")\n",
    "            print(grad_out[0])\n",
    "            self.gradients = grad_out[0]\n",
    "        first_layer = list(self.model._modules.items())[0][1]\n",
    "        hook = first_layer.register_backward_hook(hook_function)\n",
    "        return hook\n",
    "    def generate_gradient(self, inputs, labels):\n",
    "        self.model.zero_grad()\n",
    "        _ = model(inputs)\n",
    "        loss = self.loss(inputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        self.hook.remove()\n",
    "        return self.gradients\n",
    "    \n",
    "\n",
    "grad = torch.zeros((2412,400))\n",
    "i = 0\n",
    "\n",
    "\n",
    "for data in trainloader:\n",
    "    # get the inputs\n",
    "    inputs, labels = data\n",
    "    print(inputs.shape, labels.shape)\n",
    "    GG = gradient_generator(model)\n",
    "    temp = GG.generate_gradient(inputs, labels)\n",
    "    grad[i,:] = temp\n",
    "    i += 1\n",
    "\n",
    "grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
